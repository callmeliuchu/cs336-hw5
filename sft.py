"""Problem (sft_experiment): Run SFT on the MATH dataset (2 points) (2 H100 hrs)
1. Run SFT on the reasoning SFT examples (provided in /data/a5-alignment/MATH/sft.jsonl)
using the Qwen 2.5 Math 1.5B base model, varying the number of unique examples for SFT in
14
the range {128, 256, 512, 1024}, along with using the full dataset. Tune the learning rate and
batch size to achieve at least 15% validation accuracy when using the full dataset.
Deliverable: Validation accuracy curves associated with different dataset sizes.
2. Filter the reasoning SFT examples to only include examples that produce the correct answer. Run
SFT on the (full) filtered dataset and report the size of the filtered dataset and the validation
accuracy you achieve.
Deliverable: Report the size of the dataset and the validation accuracy curve you achieve.
Compare your findings to the previous SFT experiment."""
import json
import torch
import random
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from rl import tokenize_prompt_and_output,get_response_log_probs,sft_microbatch_train_step,load_policy_into_vllm_instance,init_vllm


R1_ZERO_PROMPT = open('cs336_alignment/prompts/r1_zero.prompt', 'r').read()

def tokenize_prompt_and_output_with_limit(prompt_strs, output_strs, tokenizer, device=None, max_length=2048):
    """Tokenize with sequence length limit and monitoring"""
    prompt_tokens = tokenizer(prompt_strs, truncation=True, max_length=max_length//2)['input_ids']
    output_tokens = tokenizer(output_strs, truncation=True, max_length=max_length//2)['input_ids']

    batch_sz = len(prompt_tokens)
    prompt_and_output_lens = [len(p) + len(o) for p, o in zip(prompt_tokens, output_tokens)]
    
    # ÁõëÊéßÂ∫èÂàóÈïøÂ∫¶
    max_seq_len = max(prompt_and_output_lens)
    avg_seq_len = sum(prompt_and_output_lens) / len(prompt_and_output_lens)
    print(f"Batch sequence lengths - Max: {max_seq_len}, Avg: {avg_seq_len:.1f}")
    
    # Â¶ÇÊûúË∂ÖËøáÈôêÂà∂ÔºåËøõ‰∏ÄÊ≠•Êà™Êñ≠
    if max_seq_len > max_length:
        print(f"‚ö†Ô∏è  Sequence length {max_seq_len} exceeds limit {max_length}, truncating...")
        # ÈáçÊñ∞Êà™Êñ≠Âà∞Êõ¥Áü≠ÁöÑÈïøÂ∫¶
        prompt_tokens = tokenizer(prompt_strs, truncation=True, max_length=max_length//3)['input_ids']
        output_tokens = tokenizer(output_strs, truncation=True, max_length=max_length//3)['input_ids']
        prompt_and_output_lens = [len(p) + len(o) for p, o in zip(prompt_tokens, output_tokens)]
        max_seq_len = max(prompt_and_output_lens)
        print(f"After truncation - Max: {max_seq_len}, Avg: {sum(prompt_and_output_lens)/len(prompt_and_output_lens):.1f}")
    
    padded_len = max(prompt_and_output_lens)

    # Â¶ÇÊûúÊåáÂÆö‰∫ÜËÆæÂ§áÔºåÂú®ÊåáÂÆöËÆæÂ§á‰∏äÂàõÂª∫tensor
    if device is not None:
        input_ids = torch.empty((batch_sz, padded_len - 1), dtype=torch.long, device=device)
        labels = torch.empty((batch_sz, padded_len - 1), dtype=torch.long, device=device)
        response_mask = torch.zeros((batch_sz, padded_len - 1), dtype=torch.bool, device=device)
    else:
        input_ids = torch.empty((batch_sz, padded_len - 1), dtype=torch.long)
        labels = torch.empty((batch_sz, padded_len - 1), dtype=torch.long)
        response_mask = torch.zeros((batch_sz, padded_len - 1), dtype=torch.bool)

    for i, (p_toks, o_toks) in enumerate(zip(prompt_tokens, output_tokens)):
        if device is not None:
            p_o_concat = torch.tensor(p_toks + o_toks, device=device)
        else:
            p_o_concat = torch.tensor(p_toks + o_toks)
        concat_len = len(p_o_concat)
        p_o_concat_padded = torch.nn.functional.pad(p_o_concat, (0, padded_len - concat_len), 'constant', tokenizer.eos_token_id)

        input_ids[i] = p_o_concat_padded[:-1]
        labels[i] = p_o_concat_padded[1:]

        o_start = len(p_toks) - 1
        o_end = concat_len - 1
        response_mask[i, o_start:o_end] = True
    
    return {
        'input_ids': input_ids,
        'labels': labels,
        'response_mask': response_mask,
    }

def load_math_data(data_path):
    """Load MATH dataset from jsonl file"""
    data = []
    with open(data_path, 'r') as f:
        for line in f:
            data.append(json.loads(line))
    return data

def evaluate_with_vllm(val_prompts, val_answers, epoch, policy_model, llm):
    """Evaluate model using VLLM on CUDA 1"""
    try:
        from vllm import SamplingParams
        import re
        
        # Set model to evaluation mode
        policy_model.eval()
        
        # Load the trained policy parameters into VLLM
        load_policy_into_vllm_instance(policy_model, llm)
        
        # Sample a subset for evaluation (to speed up and save memory)
        eval_indices = random.sample(range(len(val_prompts)), min(len(val_prompts), 5))  # Reduce to 20 samples
        eval_prompts = [val_prompts[i] for i in eval_indices]
        eval_answers = [val_answers[i] for i in eval_indices]
        
        # Generate responses
        sampling_params = SamplingParams(temperature=0.0, max_tokens=512)
        outputs = llm.generate(eval_prompts, sampling_params)
        
        # Calculate accuracy and show examples
        correct = 0
        print(f"\n=== Epoch {epoch} - Validation Examples ===")
        
        for i, output in enumerate(outputs):
            generated_text = output.outputs[0].text.strip()
            true_answer = eval_answers[i].strip()
            
            # Extract the final answer (assuming it's in a boxed format)
            answer_match = re.search(r'\\boxed\{([^}]+)\}', generated_text)
            if answer_match:
                predicted_answer = answer_match.group(1).strip()
                is_correct = predicted_answer == true_answer
                if is_correct:
                    correct += 1
            else:
                predicted_answer = "No boxed answer found"
                is_correct = False
            
            # Show first 3 examples
            if i < 3:
                print(f"\n--- Example {i+1} ---")
                print(f"Question: {eval_prompts[i][:200]}...")
                print(f"Generated Response: {generated_text[:300]}...")
                print(f"True Answer: {true_answer}")
                print(f"Predicted Answer: {predicted_answer}")
                print(f"Correct: {is_correct}")
        
        accuracy = correct / len(eval_prompts)
        print(f"\nEpoch {epoch} - Validation Accuracy: {accuracy:.4f} ({correct}/{len(eval_prompts)})")
        print("=" * 50)
        

        
    except ImportError:
        print("VLLM not available, skipping evaluation")
    except Exception as e:
        print(f"Evaluation error: {e}")
    finally:
        # Set model back to training mode after evaluation
        policy_model.train()
        # Ensure CUDA_VISIBLE_DEVICES is restored even if an error occurs
        pass

def sft_experiment():
    # Initialize VLLM for evaluation (only once)
    print("Initializing VLLM for evaluation...")
    from vllm import LLM
    



    # Ê£ÄÊü•ÊòØÂê¶ÊúâÂ∑≤ÂæÆË∞ÉÂ•ΩÁöÑÊ®°Âûã
    model_paths = [
        "sft_model",  # ÂæÆË∞ÉÊ®°ÂûãË∑ØÂæÑ
    ]
    
    loaded_model_path = None
    for model_path in model_paths:
        if os.path.exists(model_path) and os.path.exists(os.path.join(model_path, "config.json")):
            print(f"üîç Found existing model at: {model_path}")
            try:
                model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float16,
                    device_map="cuda:1",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                loaded_model_path = model_path
                print(f"‚úÖ Successfully loaded existing model from: {model_path}")
                break
            except Exception as e:
                print(f"‚ùå Failed to load model from {model_path}: {e}")
                continue
    
    # Â¶ÇÊûúÊ≤°ÊúâÊâæÂà∞Â∑≤ÂæÆË∞ÉÁöÑÊ®°ÂûãÔºåÂä†ËΩΩÂéüÂßãÊ®°Âûã
    if loaded_model_path is None:
        print("üÜï No existing model found, loading base model...")
        try:
            model = AutoModelForCausalLM.from_pretrained(
                "Qwen/Qwen2.5-Math-1.5B",
                torch_dtype=torch.float16,  # ‰ΩøÁî®ÂçäÁ≤æÂ∫¶‰ª•ËäÇÁúÅÊòæÂ≠ò
                device_map="cuda:1",  # ÊåáÂÆö‰ΩøÁî®GPU 1
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            print("‚úÖ Base model loaded successfully")
        except Exception as e:
            print(f"‚ùå Base model loading failed: {e}")
            raise
    else:
        print(f"üéØ Continuing training from: {loaded_model_path}")
    
    # ÂàùÂßãÂåñvLLMÊé®ÁêÜÊ®°Âûã
    print(f"Initializing vLLM inference model...")
    try:
        vllm_model = init_vllm(
            model_id="Qwen/Qwen2.5-Math-1.5B",
            gpu_memory_utilization=0.8  # Â¢ûÂä†vLLMÊòæÂ≠ò‰ΩøÁî®Áéá
        )
        print("‚úì vLLM inference model initialized successfully")
    except Exception as e:
        print(f"vLLM initialization failed: {e}")
        raise
    
    # Load tokenizer first
    tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-Math-1.5B')
    
    # Load training and validation data
    train_data = load_math_data('data/MATH/train.jsonl')
    val_data = load_math_data('data/MATH/validation.jsonl')
    
    # Prepare training prompts and answers
    train_prompts = []
    train_answers = []
    
    print("Preparing training data and analyzing sequence lengths...")
    sequence_lengths = []
    
    for i, p in enumerate(train_data):
        prompt_string = R1_ZERO_PROMPT.format(
            question=p['problem']
        )
        train_prompts.append(prompt_string)
        train_answers.append(p['answer'])
        
        # ÂàÜÊûêÂ∫èÂàóÈïøÂ∫¶ÔºàÂè™ÂàÜÊûêÂâç1000‰∏™Ê†∑Êú¨‰ª•ËäÇÁúÅÊó∂Èó¥Ôºâ
        prompt_tokens = tokenizer(prompt_string)['input_ids']
        answer_tokens = tokenizer(p['answer'])['input_ids']
        total_length = len(prompt_tokens) + len(answer_tokens)
        sequence_lengths.append(total_length)
    
    # ÊâìÂç∞Â∫èÂàóÈïøÂ∫¶ÁªüËÆ°‰ø°ÊÅØ
    if sequence_lengths:
        sequence_lengths.sort()
        print(f"Sequence length statistics (first 1000 samples):")
        print(f"  Min: {min(sequence_lengths)}")
        print(f"  Max: {max(sequence_lengths)}")
        print(f"  Mean: {sum(sequence_lengths)/len(sequence_lengths):.1f}")
        print(f"  Median: {sequence_lengths[len(sequence_lengths)//2]}")
        print(f"  95th percentile: {sequence_lengths[int(len(sequence_lengths)*0.95)]}")
        print(f"  99th percentile: {sequence_lengths[int(len(sequence_lengths)*0.99)]}")
        
        # Âª∫ËÆÆÂêàÈÄÇÁöÑÊúÄÂ§ßÂ∫èÂàóÈïøÂ∫¶ÔºàÂà©Áî®Êõ¥Â§öÊòæÂ≠òÔºâ
        suggested_max_len = min(4096, sequence_lengths[int(len(sequence_lengths)*0.98)])  # ‰ΩøÁî®98%ÂàÜ‰ΩçÊï∞
        print(f"  Suggested max length: {suggested_max_len}")
    else:
        suggested_max_len = 1024
        print(f"Using default max length: {suggested_max_len}")
    
    # Prepare validation prompts and answers
    val_prompts = []
    val_answers = []
    
    for p in val_data:
        prompt_string = R1_ZERO_PROMPT.format(
            question=p['problem']
        )
        val_prompts.append(prompt_string)
        val_answers.append(p['answer'])
    
    # ËÆæÁΩÆÊ®°Âûã‰∏∫ËÆ≠ÁªÉÊ®°Âºè
    model.train()
    
    # Ê†πÊçÆÊòØÂê¶Âä†ËΩΩ‰∫ÜÂ∑≤ÂæÆË∞ÉÊ®°ÂûãÊù•Ë∞ÉÊï¥Â≠¶‰π†Áéá
    if loaded_model_path is not None:
        # Â¶ÇÊûúÂä†ËΩΩ‰∫ÜÂ∑≤ÂæÆË∞ÉÊ®°ÂûãÔºå‰ΩøÁî®Êõ¥Â∞èÁöÑÂ≠¶‰π†ÁéáÁªßÁª≠ËÆ≠ÁªÉ
        learning_rate = 1e-3
        print(f"üìö Using reduced learning rate {learning_rate} for continued training")
    else:
        # Â¶ÇÊûúÊòØ‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉÔºå‰ΩøÁî®Ê≠£Â∏∏Â≠¶‰π†Áéá
        learning_rate = 1e-3
        print(f"üÜï Using initial learning rate {learning_rate} for new training")
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-6)

    # ËÆæÁΩÆÊúÄÂ§ßÂ∫èÂàóÈïøÂ∫¶ÈôêÂà∂ÔºàÂü∫‰∫éÊï∞ÊçÆÂàÜÊûêÁªìÊûúÔºâ
    MAX_SEQUENCE_LENGTH = suggested_max_len
    print(f"Using max sequence length: {MAX_SEQUENCE_LENGTH}")
    
    # Âä®ÊÄÅÊâπÊ¨°Â§ßÂ∞èË∞ÉÊï¥ÔºàÊõ¥‰øùÂÆàÁöÑËÆæÁΩÆÔºâ
    current_batch_size = 16  # ‰ªéÊõ¥Â∞èÁöÑÊâπÊ¨°ÂºÄÂßã
    max_batch_size = 16  # Èôç‰ΩéÊúÄÂ§ßÊâπÊ¨°Â§ßÂ∞è
    min_batch_size = 2   # ÊúÄÂ∞èÊâπÊ¨°Â§ßÂ∞è
    
    # ËÆ≠ÁªÉÁªüËÆ°
    best_loss = float('inf')
    best_epoch = 0
    total_epochs_trained = 0
    
    # Â¶ÇÊûúÂä†ËΩΩ‰∫ÜÂ∑≤ÂæÆË∞ÉÊ®°Âûã
    if loaded_model_path is not None:
        print(f"üìä Resuming training from existing model")
    
    print(f"üöÄ Starting training...")
    print(f"üìà Current batch size: {current_batch_size}")
    print(f"üìè Max sequence length: {MAX_SEQUENCE_LENGTH}")
    print(f"üéØ Learning rate: {learning_rate}")
    print("=" * 60)
    
    for epoch in range(200000):
        # Clear GPU cache before each epoch
        torch.cuda.empty_cache()
        
        # Randomly sample training prompts and answers (Âä®ÊÄÅÊâπÊ¨°Â§ßÂ∞è)
        indices = random.sample(range(len(train_prompts)), min(len(train_prompts), current_batch_size))
        prompt_strs = [train_prompts[i] for i in indices]
        output_strs = [train_answers[i] for i in indices]
        
        model_device = next(model.parameters()).device
        
        # ‰ΩøÁî®Â∏¶Â∫èÂàóÈïøÂ∫¶ÈôêÂà∂ÁöÑtokenizeÂáΩÊï∞
        res = tokenize_prompt_and_output_with_limit(
            prompt_strs, output_strs, tokenizer, 
            device=model_device, max_length=MAX_SEQUENCE_LENGTH
        )
        
        input_ids = res['input_ids']
        labels = res['labels']
        response_mask = res['response_mask']
        
        # Ê£ÄÊü•ÊòæÂ≠ò‰ΩøÁî®ÊÉÖÂÜµ
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(model_device) / 1024**3  # GB
            reserved = torch.cuda.memory_reserved(model_device) / 1024**3   # GB
            total_memory = torch.cuda.get_device_properties(model_device).total_memory / 1024**3  # GB
            utilization = (allocated / total_memory) * 100
            print(f"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB, Total: {total_memory:.2f}GB, Utilization: {utilization:.1f}%")
            
            # Âä®ÊÄÅË∞ÉÊï¥ÊâπÊ¨°Â§ßÂ∞èÔºàÊõ¥‰øùÂÆàÁöÑÁ≠ñÁï•Ôºâ
            if utilization < 40 and current_batch_size < max_batch_size:
                # Âè™ÊúâÂú®ÊòæÂ≠ò‰ΩøÁî®ÁéáÂæà‰ΩéÊó∂ÊâçÂ¢ûÂä†ÊâπÊ¨°Â§ßÂ∞èÔºå‰∏îÊØèÊ¨°Âè™Â¢ûÂä†1
                current_batch_size = min(max_batch_size, current_batch_size + 1)
                print(f"üìà Increasing batch size to {current_batch_size} (utilization: {utilization:.1f}%)")
            elif utilization > 70 and current_batch_size > min_batch_size:
                # ÊòæÂ≠ò‰ΩøÁî®ÁéáËæÉÈ´òÊó∂Á´ãÂç≥ÂáèÂ∞ëÊâπÊ¨°Â§ßÂ∞è
                current_batch_size = max(min_batch_size, current_batch_size - 2)
                print(f"üìâ Decreasing batch size to {current_batch_size} (utilization: {utilization:.1f}%)")
        
        try:
            # Âú®ËÆ°ÁÆóÂâçÊ£ÄÊü•ÊòæÂ≠òÔºåÂ¶ÇÊûú‰ΩøÁî®ÁéáËøáÈ´òÂàôË∑≥Ëøá
            if torch.cuda.is_available():
                current_allocated = torch.cuda.memory_allocated(model_device) / 1024**3
                current_utilization = (current_allocated / total_memory) * 100
                if current_utilization > 80:
                    print(f"‚ö†Ô∏è  High memory usage ({current_utilization:.1f}%), skipping this batch")
                    continue
            
            policy_log_probs = get_response_log_probs(model, input_ids, labels)['log_probs']
            optimizer.zero_grad()
            loss, _ = sft_microbatch_train_step(policy_log_probs, response_mask, gradient_accumulation_steps=1, normalize_constant=1.0)
            optimizer.step()
            
            # Êõ¥Êñ∞ËÆ≠ÁªÉÁªüËÆ°
            current_loss = loss.item()
            total_epochs_trained += 1
            
            # Ê£ÄÊü•ÊòØÂê¶ÊòØÊúÄ‰Ω≥Ê®°Âûã
            if current_loss < best_loss:
                best_loss = current_loss
                best_epoch = total_epochs_trained
                print(f'üèÜ New best model! Epoch {total_epochs_trained}, Loss: {current_loss:.4f}')
                
                # ‰øùÂ≠òÊúÄ‰Ω≥Ê®°ÂûãÂà∞ sft_model
                model.save_pretrained('sft_model')
                tokenizer.save_pretrained('sft_model')
                print(f'üíæ Best model saved to sft_model')
            else:
                print(f'Epoch {total_epochs_trained}, Loss: {current_loss:.4f} (Best: {best_loss:.4f} @ Epoch {best_epoch})')
        except torch.cuda.OutOfMemoryError as e:
            print(f"‚ùå OOM at epoch {epoch}: {e}")
            print("Skipping this batch and reducing parameters...")
            
            # Ê∏ÖÁêÜÊòæÂ≠ò
            torch.cuda.empty_cache()
            import gc
            gc.collect()
            
            # ÂáèÂ∞ëÊâπÊ¨°Â§ßÂ∞è
            current_batch_size = max(min_batch_size, current_batch_size // 2)
            print(f"üìâ Reduced batch size to {current_batch_size}")
            
            # ÂáèÂ∞ëÂ∫èÂàóÈïøÂ∫¶
            MAX_SEQUENCE_LENGTH = max(256, MAX_SEQUENCE_LENGTH // 2)
            print(f"üìè Reduced max sequence length to {MAX_SEQUENCE_LENGTH}")
            
            continue
        
        # Clear intermediate variables to free memory
        del input_ids, labels, response_mask, policy_log_probs, loss
        
        # Êõ¥È¢ëÁπÅÁöÑÊòæÂ≠òÊ∏ÖÁêÜ
        if epoch % 10 == 0:
            torch.cuda.empty_cache()
            import gc
            gc.collect()
        
        if (epoch+1) % 200 == 0:
            # Save model
            model.save_pretrained('sft_model')
            tokenizer.save_pretrained('sft_model')
            print(f'üíæ Model saved to sft_model')
        
        if (epoch+1) % 2 == 0:
            # Clear GPU cache before evaluation
            torch.cuda.empty_cache()
            
        
        if (epoch+1) % 100 == 0:
            # Evaluate on validation set using VLLM
            evaluate_with_vllm(val_prompts, val_answers, epoch, model, vllm_model)
            
            # Clear GPU cache after evaluation
            torch.cuda.empty_cache()

if __name__ == '__main__':
    sft_experiment()